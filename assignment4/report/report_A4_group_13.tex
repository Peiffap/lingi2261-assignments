\documentclass[journal,onecolumn]{IEEEtran}
%\usepackage[left=2.2cm,right=2.2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}
\usepackage{booktabs}
%\usepackage{commath}
\usepackage{float}
\usepackage{mathtools}
\usepackage{color}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{bm}


\usepackage[binary-units=true]{siunitx}

\newcommand{\py}[1]{\mintinline{python}{#1}}

\title{Artificial Intelligence (\texttt{LINGI2261}) \\ Assignment 4 --- Group 13}
\author{Martin Braquet, Gilles Peiffer}
\date{December 11, 2019}

\begin{document}

\maketitle

\section{The Bin Packing Problem}
\begin{enumerate}
	\item The bin packing problem can be formulated as a local search problem as follows:
	\begin{itemize}
		\item The \emph{problem} is to find a partition of a finite set \(U\) of items of rational size \(0 \le s(u) \le 1\) for each \(u \in U\) into disjoint subsets \(U_1, \ldots, U_k\) such that the sum of the item sizes in each \(U_i\) is no more than 1, and such that \(k\) is as small as possible.
		
		This is equivalent to the problem of packing a finite set of items of integer size (\(s(u) \in [0, C]\)) into bins of capacity \(C\), while minimizing the number of bins.
		
		A state is represented by a list of bins, each containing the indexes of the blocks inside this bin.
		\item The \emph{cost function}, which we want to maximize, is
		\[
		f(k, \bm{\mathrm{fullness}}) = -\mathrm{Fitness} =  \left(\frac{\sum_{i=1}^{k} \left(\frac{\mathrm{fullness}_i}{C}\right)^2}{k}\right) - 1,
		\]
		where \(k\) is the number of bins, \(\bm{\mathrm{fullness}}\) is a vector of size \(k\) containing the total weight of each bin and \(C\) is the capacity of the bins.
		This is the opposite of the Fitness coefficient, where the sign originates from a desire to have a maximization problem.
		\item A solution is feasible if every item is in a bin and
		\[
		\mathrm{fullness}_i \le C \quad \textnormal{for} \quad i = 1, \ldots, k.
		\]
		We call the set of feasible bin packings \(\mathcal{S}\).
		\item A solution \(\sigma\) is optimal if
		\[
		s \in \mathcal{S} \implies f(\sigma) \ge f(s).
		\]
		Equivalently, one can write the set of optimal solutions as \(\{s \in \mathcal{S} : f(s) = \max_{p \in \mathcal{S}} f(p)\}\).
	\end{itemize}
	\item The initial solution is constructed by taking the various items in the order in which they are given, and to try, for each one, to add it to the current bin.
	If this is possible (i.e., the bin's capacity is not exceeded), then add it and move to the next bin.
	If not, create a new bin and add the item to that bin, then move to the next item.
	This is the strategy which was initially implemented in the code template.
	
	The successor functions works by generating the two kinds of swap moves, and yielding the resulting states.
	That is, given an input state, it tries to find all possible item-item and item-blank space swaps which do not violate the feasibility constraints and yields a successor state where the bins have been updated to reflect the change.
	
	Both functions have been submitted and verified on INGInious, obtaining a score of 15/15.
	\item  The results for the three strategies on each of the ten given instances are detailed in Table~\ref{time1}.\footnote{Experiments were run on an Early 2015 MacBook Pro, running macOS Sierra 10.12.6, using a \SI{2.9}{\giga\hertz} Intel Core i5 processor, with \SI{8}{\giga\byte} of \SI{1867}{\mega\hertz} DDR3 RAM and an Intel Iris Graphics 6100 GPU.}
	When applicable, instances were tested multiple times and the average values were taken.
	The optimality value is given using the cost function detailed higher up in this document.
	When the sign is reversed, this becomes the fitness measure defined in the assignment's statement.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{c@{\hspace{0.7cm}}ccc|ccc|ccc} 
			\toprule
			Inst. & \multicolumn{3}{c}{\py{maxvalue}} & \multicolumn{3}{c}{\py{randomized_maxvalue}} & \multicolumn{3}{c}{\py{random_walk}}  \\
			\midrule
			& Time (ms) & Opt. & Steps & Time (ms) & Opt. & Steps & Time (ms) & Opt. & Steps \\
			\midrule
			1 & 213 & \(-0.0256572\) & 9 & 214 & \(-0.0487638\) & 13.1 & 401 & \(-0.314751\) & 12.7 \\ 
			2 & 329 & \(-0.0316317\) & 10 & 242 & \(-0.0316621\) & 32.6 & 430 & \(-0.296054\) & 2.3 \\ 
			3 & 252 & \(-0.231149\) & 5 & 261 & \(-0.231456\) & 15.6 & 409 & \(-0.291955\) & 4.8 \\ 
			4 & 251 & \(-0.233704\) & 6 & 260 & \(-0.234356\) & 31.5 & 450 & \(-0.295092\) & 1.6 \\ 
			5 & 546 & \(-0.247307\) & 2 & 357 & \(-0.247385\) & 2.7 & 386 & \(-0.247858\) & 0 \\ 
			6 & 183 & \(-0.0278449\) & 6 & 219 & \(-0.0278968\) & 28 & 375 & \(-0.259277\) & 2.2 \\ 
			7 & 197 & \(-0.0417206\) & 4 & 223 & \(-0.0417037\) & 27.4 & 389 & \(-0.272147\) & 1.3 \\ 
			8 & 240 & \(-0.0370099\) & 4 & 232 & \(-0.037048\) & 5.7 & 385 & \(-0.267996\) & 2.6 \\ 
			9 & 177 & \(-0.0143129\) & 5 & 186 & \(-0.0143188\) & 14.2 & 398 & \(-0.252326\) & 0.6 \\ 
			10 & 237 & \(-0.0432446\) & 5 & 222 & \(-0.0432216\) & 25.3 & 434 & \(-0.2732\) & 1.9 \\
			\bottomrule
			\\
		\end{tabular}
		\caption{Comparison of execution time, optimal value and number of steps for the three local search strategies.}
		\label{time1}
	\end{table}
	\item \begin{enumerate}
		\item It is hard to say which strategy is best, since the results for the \py{maxvalue} and \py{randomized_maxvalue} strategies are very similar, with \py{maxvalue} obtaining its optimal value after fewer steps.
		This is slightly counterintuitive, as one expects that the \py{randomized_maxvalue} strategy would outperform the \py{maxvalue} strategy, which is likely to get stuck in local maxima.
		This is probably a byproduct of the low limit on the number of steps.
		
		There is however no doubt about the fact that both strategies outperform the \py{random_walk} strategy.
		\item The \py{maxvalue} strategy is a simple hill-climbing algorithm, and chooses the best neighbour at each step.
		The other strategies, while less likely to get stuck in local optima, do not necessarily go for the neighbours with the highest value, and thus often end up with worse solutions.
		\item The \py{maxvalue} strategy focuses entirely on intensification, that is, searching for the neighbour with the highest value, but does not take into account diversification (deviating from optimality in order to avoid getting stuck in local optima).
		
		The \py{random_walk} strategy, on the other hand, focuses entirely on diversification, while not paying attention to intensification: the next neighbour is chosen randomly, regardless of its value.
		
		Finally, the \py{randomized_maxvalue} strategy tries to find a balance between looking at the values of the neighbours (by taking the best five) and diversifying between those neighbours (by randomizing its choice).
		\item Since the \py{maxvalue} strategy does not diversify its choices, it has little to no chance of escaping local maxima.
		Unless the best neighbour of the local maximum has another neighbour which has an even higher value, the algorithm is going to start jumping back and forth between the maximum and its best neighbour, until the step limit is reached.
		
		On the other hand, the \py{random_walk} strategy is more likely to escape local maxima since it has a nonzero probability of ``randomly walking'' down the hill it climbed, finding another hill to climb.
		
		Finally, the \py{randomized_maxvalue} strategy is somewhere in between.
		It has a possibility to escape local maxima, given that the local maxima are not too isolated (so that the random part of the algorithm has a chance of jumping to a new hill).
	\end{enumerate}
\end{enumerate}

\section{Propositional Logic}
\subsection{Models and Logical Connectives}
\begin{enumerate}
	% TODO perhaps we should use a slightly less exhaustive algorithm to answer this question.
	\item The first sentence's truth table is given in Table~\ref{tab:tt1}.
	Since it does not appear in the expression, we omit \(D\).
	\begin{table}[H]
		\centering
		\(
		\begin{array}{ccc@{\quad}ccc}
		\toprule
		A & B & C & \lnot A  \lor C & \lnot B \lor C & (\lnot A  \lor C) \land (\lnot B \lor C)\\
		\midrule
		0 & 0 & 0 & 1 & 1 & 1 \\
		0 & 0 & 1 & 1 & 1 & 1 \\
		0 & 1 & 0 & 1 & 0 & 0 \\
		0 & 1 & 1 & 1 & 1 & 1 \\
		1 & 0 & 0 & 0 & 1 & 0 \\
		1 & 0 & 1 & 1 & 1 & 1 \\
		1 & 1 & 0 & 0 & 0 & 0 \\
		1 & 1 & 1 & 1 & 1 & 1 \\
		\bottomrule\\
		\end{array}
		\)
	\caption{Truth table for \((\lnot A  \lor C) \land (\lnot B \lor C)\).}
	\label{tab:tt1}
	\end{table}
	As one can see, there are ten valid interpretations (five for each value of \(D\)).
	
	The second sentence's truth table is given in Table~\ref{tab:tt2}.
	Since it does not appear in the expression, we omit \(D\).
	\begin{table}[H]
		\centering
		\(
		\begin{array}{ccc@{\quad}ccc}
		\toprule
		A & B & C & C \implies \lnot A & \lnot(B \lor C) & (C \implies \lnot A) \land \lnot(B \lor C)\\
		\midrule
		0 & 0 & 0 & 1 & 1 & 1 \\
		0 & 0 & 1 & 1 & 0 & 0 \\
		0 & 1 & 0 & 1 & 0 & 0 \\
		0 & 1 & 1 & 1 & 0 & 0 \\
		1 & 0 & 0 & 1 & 1 & 1 \\
		1 & 0 & 1 & 0 & 0 & 0 \\
		1 & 1 & 0 & 1 & 0 & 0 \\
		1 & 1 & 1 & 0 & 0 & 0 \\
		\bottomrule\\
		\end{array}
		\)
		\caption{Truth table for \((C \implies \lnot A) \land \lnot(B \lor C)\).}
		\label{tab:tt2}
	\end{table}
	As one can see, there are four valid interpretations (two for each value of \(D\)).
	
	The third sentence's truth table is given in Table~\ref{tab:tt3}.
	\begin{table}[H]
		\centering
		\(
		\begin{array}{cccc@{\quad}cccc}
		\toprule
		A & B & C & D & \lnot A \lor B & \lnot(B \implies \lnot C) & \lnot(\lnot D \implies A) & (\lnot A \lor B) \land \lnot(B \implies \lnot C) \land \lnot(\lnot D \implies A)\\
		\midrule
		0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
		0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\
		0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
		0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 \\
		0 & 1 & 0 & 1 & 1 & 0 & 0 & 0 \\
		0 & 1 & 1 & 0 & 1 & 1 & 1 & 1 \\
		0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\
		1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
		1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
		1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
		1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
		1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 \\
		1 & 1 & 1 & 0 & 1 & 1 & 0 & 0 \\
		1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\
		\bottomrule\\
		\end{array}
		\)
		\caption{Truth table for \((\lnot A \lor B) \land \lnot(B \implies \lnot C) \land \lnot(\lnot D \implies A)\).}
		\label{tab:tt3}
	\end{table}
	As one can see, there is only one valid interpretation.
\end{enumerate}

\subsection{Color Grid Problem}
\begin{enumerate}
	\item Using propositional logic, one can express the three types of constraints fairly easily.
	We write \(n\) to denote the number of rows/columns/colors.
	
	To express the fact that a color can only appear once in a given row and column, we write
	\[
	C_{i, j, k} \implies \Bigg(\bigwedge_{0 \le \alpha < n, \alpha \ne j} \lnot C_{i, \alpha, k}\Bigg) \land \Bigg(\bigwedge_{0 \le \alpha < n, \alpha \ne i} \lnot C_{\alpha, j, k}\Bigg).
	\]
	
	For the diagonal constraints, one should notice that diagonals can be defined by the properties of the indices of the elements contained in them.
	The first type, which we call ``constant-difference'' diagonals, has the property that the difference between the elements of the diagonal is a constant value.
	The second type, which we call ``constant-sum'' diagonals, has the property that the sum of the elements of the diagonal is a constant value.
	
	To express that a color can only appear once in a given constant-difference diagonal, we write
	\[
	C_{i, j, k} \implies \bigwedge_{0 \le i+\alpha, j+\alpha < n, \alpha \ne 0} \lnot C_{i+\alpha, j+\alpha, k}.
	\]
	
	To express that a color can only appear once in a given constant-sum diagonal, we write
	\[
	C_{i, j, k} \implies \bigwedge_{0 \le i+\alpha, j-\alpha < n, \alpha \ne 0} \lnot C_{i+\alpha, j-\alpha, k}.
	\]
	
	One must also assert that each color must appear once in every row and column:
	\[
	\bigwedge_{\substack{0 \le j < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le i < n} C_{i, j, k}\Bigg) \land \bigwedge_{\substack{0 \le i < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le j < n} C_{i, j, k}\Bigg).
	\]
	
	All of these constraints must be repeated for all possible values of \((i, j, k)\).
	
	Additionally, all input values must be asserted as well:
	\[
	\bigwedge_{(i, j, k) \in \textnormal{Inputs}} C_{i, j, k}.
	\]
	\item To put the constraints given in the previous part into conjunctive normal form, one proceeds as follows:
	\begin{align*}
	&\bigwedge_{\substack{0 \le i < n \\ 0 \le j < n \\ 0 \le k < n}} \left(C_{i, j, k} \implies {\underbrace{\bigwedge_{0 \le \alpha < n, \alpha \ne j} \lnot C_{i, \alpha, k}}_{\textnormal{row}}} \land {\underbrace{\bigwedge_{0 \le \alpha < n, \alpha \ne i} \lnot C_{\alpha, j, k}}_{\textnormal{column}}}\right. \\
	&\left. \qquad \qquad \! \! {} \land {\underbrace{\bigwedge_{0 \le i+\alpha, j+\alpha < n, \alpha \ne 0} \lnot C_{i+\alpha, j+\alpha, k}}_{\textnormal{constant-difference diagonal}}}  \qquad \qquad \! \! {} \land {\underbrace{\bigwedge_{0 \le i+\alpha, j-\alpha < n, \alpha \ne 0} \lnot C_{i+\alpha, j-\alpha, k}}_{\textnormal{constant-sum diagonal}}}\right) \\
	&\qquad \qquad \! \! {}  \land {\underbrace{\bigwedge_{\substack{0 \le j < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le i < n} C_{i, j, k}\Bigg) \land \bigwedge_{\substack{0 \le i < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le j < n} C_{i, j, k}\Bigg)}_{\textnormal{color}}} \land {\underbrace{\bigwedge_{(i, j, k) \in \textnormal{Inputs}} C_{i, j, k} \vphantom{\bigwedge_{\substack{0 \le j < n \\ 0 \le k < n}}}}_{\textnormal{inputs}}} \\
	=&\bigwedge_{\substack{0 \le i < n \\ 0 \le j < n \\ 0 \le k < n}} \left({\underbrace{\bigwedge_{0 \le \alpha < n, \alpha \ne j} \Big(\lnot C_{i, j, k} \lor \lnot C_{i, \alpha, k} \Big)}_{\textnormal{row}}} \land {\underbrace{\bigwedge_{0 \le \alpha < n, \alpha \ne i} \Big(\lnot C_{i, j, k} \lor \lnot C_{\alpha, j, k}\Big)}_{\textnormal{column}}} \right.\\
	&\left. \qquad \qquad \! \! {}  \land {\underbrace{\bigwedge_{0 \le i+\alpha, j+\alpha < n, \alpha \ne 0} \Big( \lnot C_{i, j, k} \lor \lnot C_{i + \alpha, j+\alpha, k}\Big)}_{\textnormal{constant-difference diagonal}}} \land {\underbrace{\bigwedge_{0 \le i+\alpha, j-\alpha < n, \alpha \ne 0} \Big( \lnot C_{i, j, k} \lor \lnot C_{i-\alpha, j+\alpha, k} \Big)}_{\textnormal{constant-sum diagonal}}}\right)\\
	&\qquad \qquad \! \! {} \land {\underbrace{\bigwedge_{\substack{0 \le j < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le i < n} C_{i, j, k}\Bigg) \land \bigwedge_{\substack{0 \le i < n \\ 0 \le k < n}} \Bigg(\bigvee_{0 \le j < n} C_{i, j, k}\Bigg)}_{\textnormal{color}}} \land {\underbrace{\bigwedge_{(i, j, k) \in \textnormal{Inputs}} C_{i, j, k} \vphantom{\bigwedge_{\substack{0 \le j < n \\ 0 \le k < n}}}}_{\textnormal{inputs}}},
	\end{align*}
	where the last line is obtained using the distributivity and implication properties.
	As required, the final form is a conjunction of disjunctions of literals, i.e., a conjunctive normal form.
	\item The \py{cgp_solver.py} program has been submitted and verified on INGInious, obtaining a score of 10/10.
\end{enumerate}

\end{document}
